---
title: ''
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


prepare data
```{r}
dim(AP)

library(keras)
library(caret)

K <- keras::backend()
```


start with a feed-forward network with one hidden layer
```{r}

AP.pca <- prcomp(train.AP)
summary(AP.pca)

xtrain <- as.matrix(train.AP)
xtest <- as.matrix(test.AP)

model <- keras_model_sequential()
model %>%
  layer_dense(units = 75, activation = "relu", input_shape = ncol(xtrain)) %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dense(units = 25, activation = "relu") %>%
  layer_dense(units = 15, activation = "relu", name = "bottleneck") %>%
  layer_dense(units = 25, activation = "relu") %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dense(units = 75, activation = "relu") %>%
  layer_dense(units = ncol(xtrain))

# summar of model
summary(model) 


# compile model
model %>% compile(
  loss = "mean_squared_error", 
  optimizer = "adam"
)

# fit model

history <- model %>% fit(
  x = xtrain, 
  y = xtrain, 
  epochs = 100,
  verbose = 0,
  shuffle=TRUE,validation_data= list(xtest, xtest)
)




plot(history)


# evaluate the performance of the model
mse.ae2 <- evaluate(model, xtrain, xtrain)
mse.ae2

```


```{r}
ggplot(data.frame(PC1 = intermediate_output[,1], PC2 = intermediate_output[,2]), aes(x = PC1, y = PC2)) + geom_point()
```


```{r}
# PCA reconstruction
pca.recon <- function(pca, mat, k){
  mu <- matrix(rep(pca$center, nrow(pca$x)), nrow = nrow(pca$x), byrow = T)
  recon <- pca$x[,1:k] %*% t(pca$rotation[,1:k]) + mu
  mse <- mean((recon - mat)^2)
  return(list(x = recon, mse = mse))
}

xhat <- rep(NA, 100)
for(k in 1:100){
  xhat[k] <- pca.recon(AP.pca, xtrain, k)$mse
}


#Autoencoder reconstruction
ae.mse <- rep(NA, 49)
for(k in 1:49){
  modelk <- keras_model_sequential()
  modelk %>%
    layer_dense(units = 75, activation = "relu", input_shape = ncol(xtrain)) %>%
    layer_dense(units = 50, activation = "relu") %>%
    layer_dense(units = k, activation = "relu", name = "bottleneck") %>%
    layer_dense(units = 50, activation = "relu") %>%
    layer_dense(units = 75, activation = "relu") %>%
    layer_dense(units = ncol(xtrain))
  modelk %>% compile(
    loss = "mean_squared_error", 
    optimizer = "adam"
  )
  modelk %>% fit(
    x = xtrain, 
    y = xtrain, 
    epochs = 20,
    verbose = 0
  )
  ae.mse[k] <- unname(evaluate(modelk, xtrain, xtrain))
}
df <- data.frame(k = c(1:100, 1:50), mse = c(xhat, ae.mse), method = c(rep("pca", 100), rep("autoencoder", 50)))
ggplot(df, aes(x = k, y = mse, col = method)) + geom_line()
```



1D CNN 
```{r}
x_train = array_reshape(xtrain, c(dim(xtrain), 1))
x_test = array_reshape(xtest, c(dim(xtest), 1))

 
model_1d <- keras_model_sequential() 
model_1d %>% 
  layer_conv_1d(filters= 100, kernel_size= 10,  activation = "relu",  input_shape= c(100,1)) %>%
  #layer_global_max_pooling_1d() %>%
  layer_max_pooling_1d(pool_size = 2L) %>%
  layer_flatten() %>% 
  layer_dense(units = 20, activation = 'relu') %>%
  layer_dense(units = 1, activation = 'softmax')
summary(model_1d)
model_1d %>% compile(
    loss = "mean_squared_error", 
    optimizer = "adam"
)
history <- model_1d %>% fit(
  x_train, x_train, 
  epochs = 60, batch_size = 100, 
  validation_split = 0.2
)
plot(history)
model %>% evaluate(x_test, x_test)
```


Pretraining LSTM with Stacked Autoencoders:
```{r}
# defining constants
N=1000
S=10
F=20

# generating random input and target variable
x<-array(runif(N*S*F, min=-1,max=1), dim=c(N,S,F)) 
str(x)
y<-c(runif(N, min=-1,max=1)) 
str(y)

# training constants
EPOCHS=100
BATCH=100

# definign stacked autoencoder
input_timeseries <- layer_input(shape = c(S,F))
encoded_l1<- layer_dense(input_timeseries, units = F)
encoded_l2<- layer_dense(encoded_l1, units = S)
decoded_l1<- layer_dense(encoded_l2, units = S)
decoded<- layer_dense(decoded_l1, F)

# generating model for autoencoder
autoencoder <- keras_model(input_timeseries, decoded)
# generating model for encoder
model_en <<- keras_model(input_timeseries, encoded_l2)

summary(autoencoder)
summary(model_en)

autoencoder  %>% compile(loss = 'mae', optimizer='adam')

# fitting stacked autoencoder
autoencoder %>% fit(
  x = x, 
  y = x, 
  epochs = EPOCHS,
  batch_size = BATCH
)

# input layer for LSTM
lstm_input_timeseries <- layer_input(shape = c(S,F))
lstm_input = model_en(lstm_input_timeseries)


# defining LSTM
lstm_layer_l1 <- layer_lstm(units = S, input_shape = c(S,S), return_sequences = TRUE, stateful = FALSE)(lstm_input)
lstm_layer_l2 <- layer_lstm(units = 1, return_sequences = FALSE, stateful = FALSE)(lstm_layer_l1)

final_model <- keras_model(inputs=lstm_input_timeseries, outputs=lstm_layer_l2)


final_model %>% compile(loss = 'mae', optimizer='adam')

summary(final_model)

# training LSTM
final_model %>% fit(x, y, 
                   batch_size = BATCH, 
                   epochs = EPOCHS, 
                   verbose = 1, 
                   shuffle = FALSE)

```

