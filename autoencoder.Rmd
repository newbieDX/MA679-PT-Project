---
title: ''
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


prepare data
```{r}
dim(AP)

library(keras)
library(caret)

K <- keras::backend()
```


start with a feed-forward network with one hidden layer
```{r}

AP.pca <- prcomp(train.AP)
summary(AP.pca)

xtrain <- as.matrix(train.AP)
xtest <- as.matrix(test.AP)

model <- keras_model_sequential()
model %>%
  layer_dense(units = 76, activation = "relu", input_shape = ncol(xtrain)) %>%
  layer_dense(units = 55, activation = "relu", name = "bottleneck") %>%
  layer_dense(units = 76, activation = "relu") %>%
  layer_dense(units = ncol(xtrain))

# summar of model
summary(model) 


# compile model
model %>% compile(
  loss = "mean_squared_error", 
  optimizer = "adam",
  metrics = c('accuracy')
)

# fit model

history <- model %>% fit(
  x = xtrain, 
  y = xtrain, 
  epochs = 100,
  verbose = 0
)


plot(history)


# evaluate the performance of the model
mse.ae2 <- evaluate(model, xtest, xtest)
mse.ae2

yhat_normal_ae <- predict(model, xtrain, verbose=0)

intermediate_layer_model <- keras_model(inputs = model$input, outputs = get_layer(model, "bottleneck")$output)

intermediate_output <- predict(intermediate_layer_model, xtrain)

```


```{r}
ggplot(data.frame(PC6 = intermediate_output[,6], PC7 = intermediate_output[,7]), aes(x = PC6, y = PC7)) + geom_point()
```


```{r}
# PCA reconstruction
pca.recon <- function(pca, mat, k){
  mu <- matrix(rep(pca$center, nrow(pca$x)), nrow = nrow(pca$x), byrow = T)
  recon <- pca$x[,1:k] %*% t(pca$rotation[,1:k]) + mu
  mse <- mean((recon - mat)^2)
  return(list(x = recon, mse = mse))
}

xhat <- rep(NA, 100)
for(k in 1:100){
  xhat[k] <- pca.recon(AP.pca, xtrain, k)$mse
}


#Autoencoder reconstruction
ae.mse <- rep(NA, 75)
for(k in 1:75){
  modelk <- keras_model_sequential()
  modelk %>%
    layer_dense(units = 76, activation = "relu", input_shape = ncol(xtrain)) %>%
    layer_dense(units = k, activation = "relu", name = "bottleneck") %>%
    layer_dense(units = 76, activation = "relu") %>%
    layer_dense(units = ncol(xtrain))
  modelk %>% compile(
    loss = "mean_squared_error", 
    optimizer = "adam"
  )
  modelk %>% fit(
    x = xtrain, 
    y = xtrain, 
    epochs = 60,
    verbose = 0
  )
  ae.mse[k] <- unname(evaluate(modelk, xtrain, xtrain))
}
df <- data.frame(k = c(1:100, 1:75), mse = c(xhat, ae.mse), method = c(rep("pca", 100), rep("autoencoder", 75)))
ggplot(df, aes(x = k, y = mse, col = method)) + geom_line()

summary(AP.pca)
```



1D CNN 
```{r}
xtrain <- as.matrix(train.AP)
x_train = array_reshape(xtrain, c(dim(xtrain), 1))
x_test = array_reshape(xtest, c(dim(xtest), 1))

ytrain <- as.matrix(AP_stance_dr)


y_train <- array_reshape(ytrain, c(dim(ytrain), 1))

 
encoder_1d <- keras_model_sequential() 
encoder_1d %>% 
  layer_conv_1d(filters= 64, kernel_size= 10,  activation = "relu",  input_shape= c(100,1)) %>%
  layer_max_pooling_1d(pool_size = 2L) %>%
  layer_conv_1d(filters= 128, kernel_size= 10,  activation = "relu") %>%
  layer_upsampling_1d(size = 2L) %>%
  layer_flatten() %>% 
  layer_dense(units = 76, activation = 'relu') %>%
  layer_dense(units = 30, activation = 'relu', name = "bottleneck") %>%
  layer_reshape(c(30, 1)) %>%
  layer_conv_1d(filters= 128, kernel_size= 10,  activation = "relu", input_shape= c(30,1)) %>%
  layer_upsampling_1d(size = 2L) %>%
  layer_conv_1d(filters= 64, kernel_size= 10,  activation = "relu") %>%
  layer_upsampling_1d(size = 2L) %>%
  layer_flatten() %>%
  layer_dense(units = 76, activation = 'relu')%>%
  layer_dense(units = 100)
summary(encoder_1d)
encoder_1d %>% compile(
    loss = "mean_squared_error", 
    optimizer = "adam"
)

history <- encoder_1d %>% fit(
  x_train, x_train, 
  epochs = 60, batch_size = 100
)

plot(history)
encoder_1d %>% evaluate(x_test, x_test)

# extract the bottleneck layer

intermediate_layer_model <- keras_model(inputs = model_1d$input, outputs = get_layer(model_1d, "dense_8")$output)

intermediate_output <- predict(intermediate_layer_model, x_train)

```


Pretraining LSTM with Stacked Autoencoders:

```{r}
library(kerasR)
library(abind)
xtrain_ap <- as.matrix(train.AP)
xtest_ap <- as.matrix(test.AP)
x_train_ap_lstm = array_reshape(xtrain_ap, c(dim(xtrain_ap)[1],1, dim(xtrain_ap)[2]))
x_test_ap_lstm = array_reshape(xtest_ap, c(dim(xtest_ap)[1],1, dim(xtest_ap)[2]))

xtrain_ml <- as.matrix(train.ML)
xtest_ml <- as.matrix(test.ML)
x_train_ml_lstm = array_reshape(xtrain_ml, c(dim(xtrain_ml)[1],1, dim(xtrain_ml)[2]))
x_test_ml_lstm = array_reshape(xtest_ml, c(dim(xtest_ml)[1],1, dim(xtest_ml)[2]))

xtrain_v <- as.matrix(train.V)
xtest_v <- as.matrix(test.V)
x_train_v_lstm = array_reshape(xtrain_v, c(dim(xtrain_v)[1],1, dim(xtrain_v)[2]))
x_test_v_lstm = array_reshape(xtest_v, c(dim(xtest_v)[1],1, dim(xtest_v)[2]))

GRFs_train_lstm <- abind(x_train_ap_lstm,x_train_ml_lstm,x_train_v_lstm, along=2)


lstm_model <- keras_model_sequential() 
lstm_model %>% 
  layer_lstm(units = 128, activation = "relu",return_sequences = TRUE, input_shape = c(1, 100))%>%
  layer_lstm(units = 32, activation = "relu", return_sequences = FALSE)%>%
  layer_repeat_vector(1)%>%
  layer_lstm(units = 128,activation = "relu", return_sequences = TRUE)%>%
  layer_lstm(units = 64, activation = "relu", return_sequences = TRUE)%>%
  time_distributed(layer_dense(units = 100))

summary(lstm_model)
lstm_model %>% compile(
    loss = "mean_squared_error", 
    optimizer = "adam"
)

lstm_model %>% fit(
  x_train_ap_lstm, x_train_ap_lstm, 
  epochs = 100,
  verbose = 1
)

lstm_model %>% evaluate(x_test_lstm, x_test_lstm)


a <- as.matrix(expand_dims(x_train_lstm[1,,],  axis = 0))

bottlenect_model <- keras_model(inputs = lstm_model$input, outputs = get_layer(lstm_model, "lstm_99")$output)

yhat_dr <- predict(bottlenect_model, x_train_lstm, verbose=0)


recons_model <- keras_model(inputs = lstm_model$input, outputs = lstm_model$output)

yhat <- predict(recons_model, GRFs_train_lstm, verbose=0)

plot(GRFs_train_lstm[1,,][1,], type = "l", col = "red")
lines(yhat[1,,][1,], type = "l", col = "green")
lines(yhat_dr[1,,], type = "l", col = "blue")
lines(FPCAdense_AP$xiEst[1,], type = "l", col = "black")
lines(yhat_normal_ae[1,], type = "l", col = "blue")
lines(intermediate_output[1,], type = "l", col = "purple")

plot(GRFs_train_lstm[1,,][2,], type = "l", col = "red")
lines(yhat[1,,][2,], type = "l", col = "green")

plot(GRFs_train_lstm[1,,][3,], type = "l", col = "red")
lines(yhat[1,,][3,], type = "l", col = "green")


yhat <- array_reshape(yhat, c(10987, 100))


ggplot(data.frame(PC1 = yhat[,1], PC2 = yhat[,2]), aes(x = PC1, y = PC2)) + geom_point()
ggplot(data.frame(PC1 = AP_stance_dr[,1], PC2 = AP_stance_dr[,2]), aes(x = PC1, y = PC2)) + geom_point()


View(yhat_dr)
```
