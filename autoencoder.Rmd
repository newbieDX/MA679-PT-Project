---
title: ''
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


prepare data
```{r}
dim(AP)

library(keras)
library(caret)

K <- keras::backend()
```


start with a feed-forward network with one hidden layer
```{r}
#Traditional Autoencoder
set.seed(123)

dt.ml <-  sort(sample(nrow(ml_grf), nrow(ml_grf)*.7))
df_train.ml<-ml_grf[dt.ml,] %>% select(-status) %>% as.matrix()
df_test.ml<-ml_grf[-dt.ml,] %>% select(-status) %>% as.matrix()

model_try1 <- keras_model_sequential()
model_try1 %>%
  layer_dense(units = 50, activation = "relu", input_shape = ncol(df_train.ml)) %>%
  layer_dense(units = 30, activation = "relu", name = "bottleneck") %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dense(units = ncol(df_train.ml))

# summar of model
summary(model_try1) 


# compile model
model_try1 %>% compile(
  loss = "mean_squared_error", 
  optimizer = "adam"
)

# fit model

history <- model_try1 %>% fit(
  x = df_train.ml, 
  y = df_train.ml, 
  epochs = 100,
  verbose = 0
)


plot(history)


# evaluate the performance of the model
mse.ae <- evaluate(model_try1, df_test.ml, df_test.ml)
mse.ae

yhat_model_try1 <- predict(model_try1, df_test.ml, verbose=0)

plot(df_test.ml[1,], type = "l", col = "red")
lines(yhat_model_try1[1,], type = "l", col = "green")

```


```{r}
library(kerasR)
library(abind)

x_train_ml_lstm = array_reshape(df_train.ml, c(dim(df_train.ml)[1],1, dim(df_train.ml)[2]))
x_test_ml_lstm = array_reshape(df_test.ml, c(dim(df_test.ml)[1],1, dim(df_test.ml)[2]))

lstm_model <- keras_model_sequential() 
lstm_model %>% 
  layer_lstm(units = 128, activation = "relu",return_sequences = TRUE, input_shape = c(1, 100))%>%
  layer_lstm(units = 32, activation = "relu", return_sequences = FALSE)%>%
  layer_repeat_vector(1)%>%
  layer_lstm(units = 32,activation = "relu", return_sequences = TRUE)%>%
  layer_lstm(units = 128, activation = "relu", return_sequences = TRUE)%>%
  time_distributed(layer_dense(units = 100))

summary(lstm_model)
lstm_model %>% compile(
    loss = "mean_squared_error", 
    optimizer = "adam"
)

history2 <- lstm_model %>% fit(
  x_train_ml_lstm, x_train_ml_lstm, 
  epochs = 100,
  verbose = 1
)


plot(history2)

mse.lstm <- lstm_model %>% evaluate(x_test_ml_lstm, x_test_ml_lstm)
  

a <- as.matrix(expand_dims(x_train_ml_lstm[1,,],  axis = 0))

bottlenect_model <- keras_model(inputs = lstm_model$input, outputs = get_layer(lstm_model, "lstm_99")$output)

yhat_dr <- predict(bottlenect_model, x_train_lstm, verbose=0)

yhat_model_lstm <- predict(lstm_model, x_test_ml_lstm, verbose=0)

plot(x_test_ml_lstm[1,,], type = "l", col = "red")
lines(yhat_model_lstm[1,,], type = "l", col = "green")
```




```{r}
ggplot(data.frame(PC6 = intermediate_output[,6], PC7 = intermediate_output[,7]), aes(x = PC6, y = PC7)) + geom_point()
```


```{r}
# PCA reconstruction
pca.recon <- function(pca, mat, k){
  mu <- matrix(rep(pca$center, nrow(pca$x)), nrow = nrow(pca$x), byrow = T)
  recon <- pca$x[,1:k] %*% t(pca$rotation[,1:k]) + mu
  mse <- mean((recon - mat)^2)
  return(list(x = recon, mse = mse))
}

xhat <- rep(NA, 100)
for(k in 1:100){
  xhat[k] <- pca.recon(AP.pca, xtrain, k)$mse
}


#Autoencoder reconstruction
ae.mse <- rep(NA, 75)
for(k in 1:75){
  modelk <- keras_model_sequential()
  modelk %>%
    layer_dense(units = 76, activation = "relu", input_shape = ncol(xtrain)) %>%
    layer_dense(units = k, activation = "relu", name = "bottleneck") %>%
    layer_dense(units = 76, activation = "relu") %>%
    layer_dense(units = ncol(xtrain))
  modelk %>% compile(
    loss = "mean_squared_error", 
    optimizer = "adam"
  )
  modelk %>% fit(
    x = xtrain, 
    y = xtrain, 
    epochs = 60,
    verbose = 0
  )
  ae.mse[k] <- unname(evaluate(modelk, xtrain, xtrain))
}
df <- data.frame(k = c(1:100, 1:75), mse = c(xhat, ae.mse), method = c(rep("pca", 100), rep("autoencoder", 75)))
ggplot(df, aes(x = k, y = mse, col = method)) + geom_line()

summary(AP.pca)
```



1D CNN 
```{r}
xtrain <- as.matrix(train.AP)
x_train = array_reshape(xtrain, c(dim(xtrain), 1))
x_test = array_reshape(xtest, c(dim(xtest), 1))

ytrain <- as.matrix(AP_stance_dr)


y_train <- array_reshape(ytrain, c(dim(ytrain), 1))

 
encoder_1d <- keras_model_sequential() 
encoder_1d %>% 
  layer_conv_1d(filters= 64, kernel_size= 10,  activation = "relu",  input_shape= c(100,1)) %>%
  layer_max_pooling_1d(pool_size = 2L) %>%
  layer_conv_1d(filters= 128, kernel_size= 10,  activation = "relu") %>%
  layer_upsampling_1d(size = 2L) %>%
  layer_flatten() %>% 
  layer_dense(units = 76, activation = 'relu') %>%
  layer_dense(units = 30, activation = 'relu', name = "bottleneck") %>%
  layer_reshape(c(30, 1)) %>%
  layer_conv_1d(filters= 128, kernel_size= 10,  activation = "relu", input_shape= c(30,1)) %>%
  layer_upsampling_1d(size = 2L) %>%
  layer_conv_1d(filters= 64, kernel_size= 10,  activation = "relu") %>%
  layer_upsampling_1d(size = 2L) %>%
  layer_flatten() %>%
  layer_dense(units = 76, activation = 'relu')%>%
  layer_dense(units = 100)
summary(encoder_1d)
encoder_1d %>% compile(
    loss = "mean_squared_error", 
    optimizer = "adam"
)

history <- encoder_1d %>% fit(
  x_train, x_train, 
  epochs = 60, batch_size = 100
)

plot(history)
encoder_1d %>% evaluate(x_test, x_test)

# extract the bottleneck layer

intermediate_layer_model <- keras_model(inputs = model_1d$input, outputs = get_layer(model_1d, "dense_8")$output)

intermediate_output <- predict(intermediate_layer_model, x_train)

```


Pretraining LSTM with Stacked Autoencoders:

```{r}
library(kerasR)
library(abind)
xtrain_ap <- as.matrix(train.AP)
xtest_ap <- as.matrix(test.AP)
x_train_ap_lstm = array_reshape(xtrain_ap, c(dim(xtrain_ap)[1],1, dim(xtrain_ap)[2]))
x_test_ap_lstm = array_reshape(xtest_ap, c(dim(xtest_ap)[1],1, dim(xtest_ap)[2]))

xtrain_ml <- as.matrix(train.ML)
xtest_ml <- as.matrix(test.ML)
x_train_ml_lstm = array_reshape(xtrain_ml, c(dim(xtrain_ml)[1],1, dim(xtrain_ml)[2]))
x_test_ml_lstm = array_reshape(xtest_ml, c(dim(xtest_ml)[1],1, dim(xtest_ml)[2]))

xtrain_v <- as.matrix(train.V)
xtest_v <- as.matrix(test.V)
x_train_v_lstm = array_reshape(xtrain_v, c(dim(xtrain_v)[1],1, dim(xtrain_v)[2]))
x_test_v_lstm = array_reshape(xtest_v, c(dim(xtest_v)[1],1, dim(xtest_v)[2]))

GRFs_train_lstm <- abind(x_train_ap_lstm,x_train_ml_lstm,x_train_v_lstm, along=2)


lstm_model <- keras_model_sequential() 
lstm_model %>% 
  layer_lstm(units = 128, activation = "relu",return_sequences = TRUE, input_shape = c(1, 100))%>%
  layer_lstm(units = 32, activation = "relu", return_sequences = FALSE)%>%
  layer_repeat_vector(1)%>%
  layer_lstm(units = 128,activation = "relu", return_sequences = TRUE)%>%
  layer_lstm(units = 64, activation = "relu", return_sequences = TRUE)%>%
  time_distributed(layer_dense(units = 100))

summary(lstm_model)
lstm_model %>% compile(
    loss = "mean_squared_error", 
    optimizer = "adam"
)

lstm_model %>% fit(
  x_train_ap_lstm, x_train_ap_lstm, 
  epochs = 100,
  verbose = 1
)

lstm_model %>% evaluate(x_test_lstm, x_test_lstm)


a <- as.matrix(expand_dims(x_train_lstm[1,,],  axis = 0))

bottlenect_model <- keras_model(inputs = lstm_model$input, outputs = get_layer(lstm_model, "lstm_99")$output)

yhat_dr <- predict(bottlenect_model, x_train_lstm, verbose=0)


recons_model <- keras_model(inputs = lstm_model$input, outputs = lstm_model$output)

yhat <- predict(recons_model, GRFs_train_lstm, verbose=0)

plot(GRFs_train_lstm[1,,][1,], type = "l", col = "red")
lines(yhat[1,,][1,], type = "l", col = "green")
lines(yhat_dr[1,,], type = "l", col = "blue")
lines(FPCAdense_AP$xiEst[1,], type = "l", col = "black")
lines(yhat_normal_ae[1,], type = "l", col = "blue")
lines(intermediate_output[1,], type = "l", col = "purple")

plot(GRFs_train_lstm[1,,][2,], type = "l", col = "red")
lines(yhat[1,,][2,], type = "l", col = "green")

plot(GRFs_train_lstm[1,,][3,], type = "l", col = "red")
lines(yhat[1,,][3,], type = "l", col = "green")


yhat <- array_reshape(yhat, c(10987, 100))


ggplot(data.frame(PC1 = yhat[,1], PC2 = yhat[,2]), aes(x = PC1, y = PC2)) + geom_point()
ggplot(data.frame(PC1 = AP_stance_dr[,1], PC2 = AP_stance_dr[,2]), aes(x = PC1, y = PC2)) + geom_point()


View(yhat_dr)
```



outlier detector model:

```{r}
K <- keras::backend()

set.seed(1234)

dt <-  sort(sample(nrow(ml_grf), nrow(ml_grf)*.7))
train.ml<-ml_grf[dt,] 
test.ml<-ml_grf[-dt,] 

desc <- train.ml %>% 
  select(-status) %>% 
  get_desc()

x_train.ml <- train.ml %>%
  select(-status) %>%
  normalization_minmax(desc) %>%
  as.matrix()

x_test.ml <- test.ml %>%
  select(-status) %>%
  normalization_minmax(desc) %>%
  as.matrix()

y_train.ml <- train.ml$status
y_test.ml <- test.ml$status

model_ml <- keras_model_sequential()
model_ml %>%
  layer_dense(units = 50, activation = "relu", input_shape = ncol(x_train.ml)) %>%
  layer_dense(units = 30, activation = "relu", name = "bottleneck") %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dense(units = ncol(x_train.ml))



# summar of model
summary(model_ml) 



# compile model
model_ml %>% compile(
  loss = "mean_squared_error", 
  optimizer = "adam"
)


checkpoint <- callback_model_checkpoint(
  filepath = "model.hdf5", 
  save_best_only = TRUE, 
  period = 1,
  verbose = 1
)

early_stopping <- callback_early_stopping(patience = 5)

model_ml %>% fit(
  x = x_train.ml[y_train.ml == 0,], 
  y = x_train.ml[y_train.ml == 0,], 
  epochs = 100, 
  batch_size = 32,
  validation_data = list(x_test.ml[y_test.ml == 0,], x_test.ml[y_test.ml == 0,]), 
  callbacks = list(checkpoint, early_stopping)
)

loss <- evaluate(model_ml, x = x_test.ml[y_test.ml == 0,], y = x_test.ml[y_test.ml == 0,])
loss


pred_train.ml <- predict(model_ml, x_train.ml)
mse_train.ml <- apply((x_train.ml - pred_train.ml)^2, 1, sum)

pred_test.ml <- predict(model_ml, x_test.ml)
mse_test.ml <- apply((x_test.ml - pred_test.ml)^2, 1, sum)

possible_k <- seq(0, 0.5, length.out = 100)
precision <- sapply(possible_k, function(k) {
  predicted_class <- as.numeric(mse_test.ml > k)
  sum(predicted_class == 1 & y_test.ml == 1)/sum(predicted_class)
})

qplot(possible_k, precision, geom = "line") +
  labs(x = "Threshold", y = "Precision")


recall <- sapply(possible_k, function(k) {
  predicted_class <- as.numeric(mse_test.ml > k)
  sum(predicted_class == 1 & y_test.ml == 1)/sum(y_test.ml)
})

qplot(possible_k, recall, geom = "line") +
  labs(x = "Threshold", y = "Recall")

qplot(recall, precision, geom = "line")+
  labs(x = "Recall", y = "Precision")

fmeasure <- (2 * precision * recall) / (precision + recall)

which.max(fmeasure)

possible_k[which.max(fmeasure)]
```





