---
title: ''
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


prepare data
```{r}
dim(AP)

library(keras)
library(caret)

K <- keras::backend()
```


start with a feed-forward network with one hidden layer
```{r}

AP.pca <- prcomp(train.AP)
summary(AP.pca)

xtrain <- as.matrix(train.AP)
xtest <- as.matrix(test.AP)

model <- keras_model_sequential()
model %>%
  layer_dense(units = 76, activation = "relu", input_shape = ncol(xtrain)) %>%
  layer_dense(units = 55, activation = "relu", name = "bottleneck") %>%
  layer_dense(units = 76, activation = "relu") %>%
  layer_dense(units = ncol(xtrain))

# summar of model
summary(model) 


# compile model
model %>% compile(
  loss = "mean_squared_error", 
  optimizer = "adam",
  metrics = c('accuracy')
)

# fit model

history <- model %>% fit(
  x = xtrain, 
  y = xtrain, 
  epochs = 80,
  batch_size = 100,
  validation_data= list(xtest, xtest)
)


plot(history)


# evaluate the performance of the model
mse.ae2 <- evaluate(model, xtest, xtest)
mse.ae2

```


```{r}
ggplot(data.frame(PC1 = intermediate_output[,1], PC2 = intermediate_output[,2]), aes(x = PC1, y = PC2)) + geom_point()
```


```{r}
# PCA reconstruction
pca.recon <- function(pca, mat, k){
  mu <- matrix(rep(pca$center, nrow(pca$x)), nrow = nrow(pca$x), byrow = T)
  recon <- pca$x[,1:k] %*% t(pca$rotation[,1:k]) + mu
  mse <- mean((recon - mat)^2)
  return(list(x = recon, mse = mse))
}

xhat <- rep(NA, 100)
for(k in 1:100){
  xhat[k] <- pca.recon(AP.pca, xtrain, k)$mse
}


#Autoencoder reconstruction
ae.mse <- rep(NA, 75)
for(k in 1:75){
  modelk <- keras_model_sequential()
  modelk %>%
    layer_dense(units = 76, activation = "relu", input_shape = ncol(xtrain)) %>%
    layer_dense(units = k, activation = "relu", name = "bottleneck") %>%
    layer_dense(units = 76, activation = "relu") %>%
    layer_dense(units = ncol(xtrain))
  modelk %>% compile(
    loss = "mean_squared_error", 
    optimizer = "adam"
  )
  modelk %>% fit(
    x = xtrain, 
    y = xtrain, 
    epochs = 60,
    verbose = 0
  )
  ae.mse[k] <- unname(evaluate(modelk, xtrain, xtrain))
}
df <- data.frame(k = c(1:100, 1:75), mse = c(xhat, ae.mse), method = c(rep("pca", 100), rep("autoencoder", 75)))
ggplot(df, aes(x = k, y = mse, col = method)) + geom_line()

summary(AP.pca)
```



1D CNN 
```{r}
xtrain <- as.matrix(train.AP)
x_train = array_reshape(xtrain, c(dim(xtrain), 1))
x_test = array_reshape(xtest, c(dim(xtest), 1))

ytrain <- as.matrix(AP_stance_dr)


y_train <- array_reshape(ytrain, c(dim(ytrain), 1))

 
encoder_1d <- keras_model_sequential() 
encoder_1d %>% 
  layer_conv_1d(filters= 64, kernel_size= 10,  activation = "relu",  input_shape= c(100,1)) %>%
  layer_max_pooling_1d(pool_size = 2L) %>%
  layer_conv_1d(filters= 128, kernel_size= 10,  activation = "relu") %>%
  layer_upsampling_1d(size = 2L) %>%
  layer_flatten() %>% 
  layer_dense(units = 76, activation = 'relu') %>%
  layer_dense(units = 30, activation = 'relu', name = "bottleneck") %>%
  layer_reshape(c(30, 1)) %>%
  layer_conv_1d(filters= 128, kernel_size= 10,  activation = "relu", input_shape= c(30,1)) %>%
  layer_upsampling_1d(size = 2L) %>%
  layer_conv_1d(filters= 64, kernel_size= 10,  activation = "relu") %>%
  layer_upsampling_1d(size = 2L) %>%
  layer_flatten() %>%
  layer_dense(units = 76, activation = 'relu')%>%
  layer_dense(units = 100)
summary(encoder_1d)
encoder_1d %>% compile(
    loss = "mean_squared_error", 
    optimizer = "adam"
)

history <- encoder_1d %>% fit(
  x_train, x_train, 
  epochs = 60, batch_size = 100
)

plot(history)
encoder_1d %>% evaluate(x_test, x_test)

# extract the bottleneck layer

intermediate_layer_model <- keras_model(inputs = model_1d$input, outputs = get_layer(model_1d, "dense_8")$output)

intermediate_output <- predict(intermediate_layer_model, x_train)


dim(x_train)
```


Pretraining LSTM with Stacked Autoencoders:

