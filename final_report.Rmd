---
title: "SAR PT Project Final Report"
subtitle: "Osteoarthritis Research Analysis"
author: "Boyu Chen, Chen Xu"
date: "May 6, 2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,out.width="0.9\\linewidth",dev="pdf",fig.align  = 'center')
pacman::p_load(
  "fda.usc",
  "fdaoutlier",
  "tidyverse",
  "fda",
  "dplyr",
  "fdapace",
  "plotly",
  "readxl",
  "purrr",
  "keras",
  "caret"
)
```


```{r}
#load data:
ID_info <- read_excel("/Users/xuchen/Desktop/MA679/PTProject_Data/IDinfo.xls")
# Time-normalized data
AP_GRF_stance_N <- read.csv("/Users/xuchen/Desktop/MA679/PTProject_Data/AP_GRF_stance_N.csv", header = FALSE)
ML_GRF_stance_N <- read.csv("/Users/xuchen/Desktop/MA679/PTProject_Data/ML_GRF_stance_N.csv", header = FALSE)
V_GRF_stance_N <- read.csv("/Users/xuchen/Desktop/MA679/PTProject_Data/V_GRF_stance_N.csv", header = FALSE)
```

Abstract:




Introduction:



Explortary data analysis:

The first step: datasets select

select AP_GRFs, ML_GRFs, V_GRFs

```{r}
mat_ap <- as.matrix(AP_GRF_stance_N)
mat_ml <- as.matrix(ML_GRF_stance_N)
mat_v <- as.matrix(V_GRF_stance_N)

color = rgb(runif(15696,0,1),runif(15696,0,1), runif(15696,0,1))

ml_ts <- function(df){
  plot(df[1,], col = color[1], type = 'l', lwd = 1, ylim = c(-150,200), xlab = "Time point", ylab = "ML_GRF_stance_N")
  for (i in 2:nrow(df)) {
    lines(df[i,], col = color1[i], type = 'l', lwd = 1)
  }
}

ap_ts <- function(df){
  plot(df[1,], col = color[1], type = 'l', lwd = 1, ylim = c(-400,400), xlab = "Time point", ylab = "AP_GRF_stance_N")
  for (i in 2:nrow(df)) {
    lines(df[i,], col = color1[i], type = 'l', lwd = 1)
  }
}

v_ts <- function(df){
  plot(df[1,], col = color[1], type = 'l', lwd = 1, ylim = c(0,2000), xlab = "Time point", ylab = "V_GRF_stance_N")
  for (i in 2:nrow(df)) {
    lines(df[i,], col = color1[i], type = 'l', lwd = 1)
  }
}

```


Reconstruct the time normalized force plot:

```{r}
ap_plot <- ap_ts(mat_ap)
```

```{r}
ml_plot <- ml_ts(mat_ml)
```

```{r}
v_plot <- v_ts(mat_v)
```


Detect outliers for all 3 time normalized GRFs and label them

```{r}

ap_ms <- msplot(dts = AP_GRF_stance_N, plot = F)

ap_outlier_id <- ap_ms$outliers

ap_grf <- AP_GRF_stance_N %>%
  mutate(index = row_number())

# label the outliers

for (i in 1:nrow(ap_grf)) {
  if (ap_grf$index[i] %in% ap_outlier_id == TRUE) {
    ap_grf$status[i] = 1
  }
  if (ap_grf$index[i] %in% ap_outlier_id == FALSE) {
    ap_grf$status[i] = 0
  }
}

ap_grf <- ap_grf[c(102, 1:100)]


ml_ms <- msplot(dts = ML_GRF_stance_N, plot = F)

ml_outlier_id <- ml_ms$outliers

ml_grf <- ML_GRF_stance_N %>%
  mutate(index = row_number())

# label the outliers

for (i in 1:nrow(ml_grf)) {
  if (ml_grf$index[i] %in% ml_outlier_id == TRUE) {
    ml_grf$status[i] = 1
  }
  if (ml_grf$index[i] %in% ml_outlier_id == FALSE) {
    ml_grf$status[i] = 0
  }
}

ml_grf <- ml_grf[c(102, 1:100)]



v_ms <- msplot(dts = V_GRF_stance_N, plot = F)

v_outlier_id <- v_ms$outliers


v_grf <- V_GRF_stance_N %>%
  mutate(index = row_number())

# label the outliers

for (i in 1:nrow(v_grf)) {
  if (v_grf$index[i] %in% v_outlier_id == TRUE) {
    v_grf$status[i] = 1
  }
  if (v_grf$index[i] %in% v_outlier_id == FALSE) {
    v_grf$status[i] = 0
  }
}

v_grf <- v_grf[c(102, 1:100)]
```

Baseline model: Functional PCA

data processing

```{r}
data_pre_func <- function(df1, df2){
  df <- cbind(df1, df2)
  df3 <- df %>% pivot_longer(-c(ID, KNEE, TRIAL, tr_length), names_to = "time", values_to = "force")%>%
    mutate(new = paste(ID, "-",KNEE, "-",TRIAL)) 
  
  t <- rep(seq(0,100,length.out = 100),nrow(df))
  
  df_pre <- cbind(df3, t)
  return(df_pre)
}

ap_fpca <- data_pre_func(ID_info, AP_GRF_stance_N)
ml_fpca <- data_pre_func(ID_info, ML_GRF_stance_N)
v_fpca <- data_pre_func(ID_info, V_GRF_stance_N)
```


Do FPCA for all 5 time normalized dfs:
```{r}
L_AP <- MakeFPCAInputs(ap_fpca$new, ap_fpca$t, ap_fpca$force)
FPCAdense_AP <- FPCA(L_AP$Ly, L_AP$Lt)

# Plot the FPCA object
plot(FPCAdense_AP)

par(mfrow=c(1,1))
CreateOutliersPlot(FPCAdense_AP, optns = list(K = 12, variant = 'KDE'))
```


```{r}
L_ML <- MakeFPCAInputs(ml_fpca$new, ml_fpca$t, ml_fpca$force)
FPCAdense_ML <- FPCA(L_ML$Ly, L_ML$Lt)

# Plot the FPCA object
plot(FPCAdense_ML)

par(mfrow=c(1,1))
  CreateOutliersPlot(FPCAdense_ML, optns = list(K = 15, variant = 'KDE'))
```


```{r}
L_V <- MakeFPCAInputs(v_fpca$new, v_fpca$t, v_fpca$force)
FPCAdense_V <- FPCA(L_V$Ly, L_V$Lt)

# Plot the FPCA object
plot(FPCAdense_V)

par(mfrow=c(1,1))
  CreateOutliersPlot(FPCAdense_V, optns = list(K = 8, variant = 'KDE'))
```


Tradictional auto-encoder

We created 2 functions to help us. The first one gets descriptive statistics about the dataset that are used for scaling. Then we have a function to perform the min-max scaling.

```{r}

#' Gets descriptive statistics for every variable in the dataset.
get_desc <- function(x) {
  map(x, ~list(
    min = min(.x),
    max = max(.x),
    mean = mean(.x),
    sd = sd(.x)
  ))
} 

#' Given a dataset and normalization constants it will create a min-max normalized
#' version of the dataset.
normalization_minmax <- function(x, desc) {
  map2_dfc(x, desc, ~(.x - .y$min)/(.y$max - .y$min))
}
```


Create traditional Autoencoder model for outlier detection

Select ML_GRF_stance_N as example

```{r}

K <- keras::backend()

dt.ml <-  sort(sample(nrow(ml_grf), nrow(ml_grf)*.7))
df_train.ml<-ml_grf[dt.ap,]
df_test.ml<-ml_grf[-dt.ap,]


desc <- df_train.ml %>% 
  select(-status) %>% 
  get_desc()

x_train.ml <- df_train.ml %>%
  select(-status) %>%
  normalization_minmax(desc) %>%
  as.matrix()

x_test.ml <- df_test.ml %>%
  select(-status) %>%
  normalization_minmax(desc) %>%
  as.matrix()

y_train.ml <- df_train.ml$status
y_test.ml <- df_test.ml$status

model_ml <- keras_model_sequential()
model_ml %>%
  layer_dense(units = 50, activation = "relu", input_shape = ncol(x_train.ml)) %>%
  layer_dense(units = 30, activation = "relu", name = "bottleneck") %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dense(units = ncol(x_train.ml))



# summar of model
summary(model_ml) 



# compile model
model_ml %>% compile(
  loss = "mean_squared_error", 
  optimizer = "adam"
)


checkpoint <- callback_model_checkpoint(
  filepath = "model.hdf5", 
  save_best_only = TRUE, 
  period = 1,
  verbose = 1
)

early_stopping <- callback_early_stopping(patience = 5)

model_ml %>% fit(
  x = x_train.ml[y_train.ml == 0,], 
  y = x_train.ml[y_train.ml == 0,], 
  epochs = 100, 
  batch_size = 32,
  validation_data = list(x_test.ml[y_test.ml == 0,], x_test.ml[y_test.ml == 0,]), 
  callbacks = list(checkpoint, early_stopping)
)

loss <- evaluate(model_ml, x = x_test.ml[y_test.ml == 0,], y = x_test.ml[y_test.ml == 0,])
loss


pred_train.ml <- predict(model_ml, x_train.ml)
mse_train.ml <- apply((x_train.ml - pred_train.ml)^2, 1, sum)

pred_test.ml <- predict(model_ml, x_test.ml)
mse_test.ml <- apply((x_test.ml - pred_test.ml)^2, 1, sum)

possible_k <- seq(0, 0.5, length.out = 100)
precision <- sapply(possible_k, function(k) {
  predicted_class <- as.numeric(mse_test.ml > k)
  sum(predicted_class == 1 & y_test.ml == 1)/sum(predicted_class)
})

qplot(possible_k, precision, geom = "line") +
  labs(x = "Threshold", y = "Precision")


recall <- sapply(possible_k, function(k) {
  predicted_class <- as.numeric(mse_test.ml > k)
  sum(predicted_class == 1 & y_test.ml == 1)/sum(y_test.ml)
})

qplot(possible_k, recall, geom = "line") +
  labs(x = "Threshold", y = "Recall")

qplot(recall, precision, geom = "line")+
  labs(x = "Recall", y = "Precision")

fmeasure <- (2 * precision * recall) / (precision + recall)

which.max(fmeasure)

possible_k[which.max(fmeasure)]
```


Model Reconstruction check
```{r}
plot(x_train.ml[1,], type = "l", col = "red")
lines(pred_train.ml[1,], type = "l", col = "green")
```


Results:




Discussion

