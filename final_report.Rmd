---
title: "Untitled"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(fda.usc)
library(fdaoutlier)
library(tidyverse)
library(plotly)
library(dplyr)
library(RColorBrewer)
library(readxl)
```


```{r}
#load data:
ID_info <- read_excel("/Users/xuchen/Desktop/MA679/PTProject_Data/IDinfo.xls")
# Time-normalized data
AP_GRF_stance_N <- read.csv("/Users/xuchen/Desktop/MA679/PTProject_Data/AP_GRF_stance_N.csv", header = FALSE)
ML_GRF_stance_N <- read.csv("/Users/xuchen/Desktop/MA679/PTProject_Data/ML_GRF_stance_N.csv", header = FALSE)
V_GRF_stance_N <- read.csv("/Users/xuchen/Desktop/MA679/PTProject_Data/V_GRF_stance_N.csv", header = FALSE)
```

Abstract:




Introduction:



Explortary data analysis:

Reconstruct the time normalized force plot:

```{r}
mat_ap <- as.matrix(AP_GRF_stance_N)
mat_ml <- as.matrix(ML_GRF_stance_N)
mat_v <- as.matrix(V_GRF_stance_N)

color = rgb(runif(15696,0,1),runif(15696,0,1), runif(15696,0,1))

ml_ts <- function(df){
  plot(df[1,], col = color[1], type = 'l', lwd = 1, ylim = c(-150,200), xlab = "Time point", ylab = "ML_GRF_stance_N")
  for (i in 2:nrow(df)) {
    lines(df[i,], col = color1[i], type = 'l', lwd = 1)
  }
}

ap_ts <- function(df){
  plot(df[1,], col = color[1], type = 'l', lwd = 1, ylim = c(-400,400), xlab = "Time point", ylab = "AP_GRF_stance_N")
  for (i in 2:nrow(df)) {
    lines(df[i,], col = color1[i], type = 'l', lwd = 1)
  }
}

v_ts <- function(df){
  plot(df[1,], col = color[1], type = 'l', lwd = 1, ylim = c(0,2000), xlab = "Time point", ylab = "V_GRF_stance_N")
  for (i in 2:nrow(df)) {
    lines(df[i,], col = color1[i], type = 'l', lwd = 1)
  }
}

```

The first step: dataset select

select AP_GRFs, ML_GRFs, V_GRFs

```{r}
ap_plot <- ap_ts(mat_ap)
```

```{r}
ml_plot <- ml_ts(mat_ml)
```

```{r}
v_plot <- v_ts(mat_v)
```


Detect outliers for all 3 time normalized GRFs and label them

```{r}

ap_ms <- msplot(dts = AP_GRF_stance_N, plot = F)
ap_ms$outliers

ap_outlier_id <- ap_ms$outliers %>% as.data.frame()

ap_grf <- AP_GRF_stance_N %>%
  mutate(index = row_number())

# label the outliers

for (i in 1:nrow(ap_grf)) {
  if (ap_grf$index[i] %in% ap_outlier_id == TRUE) {
    ap_grf$status[i] = "Outlier"
  }
  if (ap_grf$index[i] %in% ap_outlier_id == FALSE) {
    ap_grf$status[i] = "Good"
  }
}

ap_grf <- ap_grf[c(102, 1:100)]

ap_outlier <- subset(ap_grf, ap_grf$status == "Outlier")



ml_ms <- msplot(dts = ML_GRF_stance_N, plot = F)

ml_outlier_id <- ml_ms$outliers

ml_grf <- ML_GRF_stance_N %>%
  mutate(index = row_number())

# label the outliers

for (i in 1:nrow(ml_grf)) {
  if (ml_grf$index[i] %in% ml_outlier_id == TRUE) {
    ml_grf$status[i] = 1
  }
  if (ml_grf$index[i] %in% ml_outlier_id == FALSE) {
    ml_grf$status[i] = 0
  }
}

ml_grf <- ml_grf[c(102, 1:100)]



v_ms <- msplot(dts = V_GRF_stance_N, plot = F)
v_ms$outliers

v_outlier_id <- v_ms$outliers %>% as.data.frame()

grf_outlier_id <- inner_join(ap_outlier_id, ml_outlier_id, by = ".")
grf_outlier_id <- inner_join(grf_outlier_id, v_outlier_id, by = ".")


```

Baseline model: Functional PCA

data processing

```{r}
dt.1 <-  sort(sample(nrow(ap_grf), nrow(ap_grf)*.7))
train.fpca.AP<-AP_GRF_stance_N[dt.1,]
test.fpca.AP<-AP_GRF_stance_N[-dt.1,]

train.ML<-ML_GRF_stance_N[dt.1,]
test.ML<-ML_GRF_stance_N[-dt.1,]

train.V<-V_GRF_stance_N[dt.1,]
test.V<-V_GRF_stance_N[-dt.1,]

train.ID <-ID_info[dt.1,]
test.ID <-ID_info[-dt.1,]


data_pre_func <- function(df1, df2){
  df <- cbind(df1, df2)
  df3 <- df %>% pivot_longer(-c(ID, KNEE, TRIAL, tr_length), names_to = "time", values_to = "force")%>%
    mutate(new = paste(ID, "-",KNEE, "-",TRIAL)) 
  
  t <- rep(seq(0,100,length.out = 100),nrow(df))
  
  df_pre <- cbind(df3, t)
  return(df_pre)
}

try <- data_pre_func(train.ID, train.fpca.AP)
try2 <- data_pre_func(train.ID, train.ML)
try3 <- data_pre_func(train.ID, train.V)
```


Do FPCA for all 5 time normalized dfs:
```{r}
L_AP <- MakeFPCAInputs(try$new, try$t, try$force)
FPCAdense_AP <- FPCA(L_AP$Ly, L_AP$Lt)

# Plot the FPCA object
plot(FPCAdense_AP)


# Select the first PCs:
AP_stance_dr <- FPCAdense_AP$xiEst
colnames(AP_stance_dr) <- c('AP_FPC1', 
                            'AP_FPC2', 
                            'AP_FPC3',
                            'AP_FPC4',
                            'AP_FPC5', 
                            'AP_FPC6', 
                            'AP_FPC7',
                            'AP_FPC8',
                            'AP_FPC9',
                            'AP_FPC10',
                            'AP_FPC11',
                            'AP_FPC12')




fpca_ap_ms <- msplot(dts = AP_stance_dr, plot = F)
fpca_ap_ms$outliers

fpca_ap_outlier_id <- fpca_ap_ms$outliers

fpca_ap_grf <- AP_stance_dr %>% as.data.frame() %>%
  mutate(index = row_number())

# label the outliers

for (i in 1:nrow(fpca_ap_grf)) {
  if (fpca_ap_grf$index[i] %in% fpca_ap_outlier_id == TRUE) {
    fpca_ap_grf$status[i] = "Outlier"
  }
  if (fpca_ap_grf$index[i] %in% fpca_ap_outlier_id == FALSE) {
    fpca_ap_grf$status[i] = "Good"
  }
}


fpc_ap.pca <- prcomp(fpca_ap_grf[1:12])

fpca_ap_plot <- autoplot(fpc_ap.pca,
         loadings = FALSE, loadings.colour = 'blue',
         loadings.label = FALSE, loadings.label.size = 3,
         data = fpca_ap_grf,
         colour = "status")


fpca_ap_plot
```


Normal auto-encoder

We created 2 functions to help us. The first one gets descriptive statistics about the dataset that are used for scaling. Then we have a function to perform the min-max scaling.

```{r}
library(purrr)

#' Gets descriptive statistics for every variable in the dataset.
get_desc <- function(x) {
  map(x, ~list(
    min = min(.x),
    max = max(.x),
    mean = mean(.x),
    sd = sd(.x)
  ))
} 

#' Given a dataset and normalization constants it will create a min-max normalized
#' version of the dataset.
normalization_minmax <- function(x, desc) {
  map2_dfc(x, desc, ~(.x - .y$min)/(.y$max - .y$min))
}
```


```{r}

library(keras)
library(caret)

K <- keras::backend()

dt.ml <-  sort(sample(nrow(ml_grf), nrow(ml_grf)*.7))
df_train.ml<-ml_grf[dt.ap,]
df_test.ml<-ml_grf[-dt.ap,]


desc <- df_train.ml %>% 
  select(-status) %>% 
  get_desc()

x_train.ml <- df_train.ml %>%
  select(-status) %>%
  normalization_minmax(desc) %>%
  as.matrix()

x_test.ml <- df_test.ml %>%
  select(-status) %>%
  normalization_minmax(desc) %>%
  as.matrix()

y_train.ml <- df_train.ml$status
y_test.ml <- df_test.ml$status

model_ml <- keras_model_sequential()
model_ml %>%
  layer_dense(units = 50, activation = "relu", input_shape = ncol(x_train.ml)) %>%
  layer_dense(units = 35, activation = "relu", name = "bottleneck") %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dense(units = ncol(x_train.ml))

# summar of model
summary(model_ml) 


# compile model
model_ml %>% compile(
  loss = "mean_squared_error", 
  optimizer = "adam",
  metrics = c('accuracy')
)


checkpoint <- callback_model_checkpoint(
  filepath = "model.hdf5", 
  save_best_only = TRUE, 
  period = 1,
  verbose = 1
)

early_stopping <- callback_early_stopping(patience = 5)

model_ml %>% fit(
  x = x_train.ml[y_train.ml == 0,], 
  y = x_train.ml[y_train.ml == 0,], 
  epochs = 100, 
  batch_size = 32,
  validation_data = list(x_test.ml[y_test.ml == 0,], x_test.ml[y_test.ml == 0,]), 
  callbacks = list(checkpoint, early_stopping)
)

loss <- evaluate(model_ml, x = x_test.ml[y_test.ml == 0,], y = x_test.ml[y_test.ml == 0,])
loss


pred_train.ml <- predict(model_ml, x_train.ml)
mse_train.ml <- apply((x_train.ml - pred_train.ml)^2, 1, sum)

pred_test.ml <- predict(model_ml, x_test.ml)
mse_test.ml <- apply((x_test.ml - pred_test.ml)^2, 1, sum)

possible_k <- seq(0, 0.5, length.out = 100)
precision <- sapply(possible_k, function(k) {
  predicted_class <- as.numeric(mse_test.ml > k)
  sum(predicted_class == 1 & y_test.ml == 1)/sum(predicted_class)
})

qplot(possible_k, precision, geom = "line") +
  labs(x = "Threshold", y = "Precision")


recall <- sapply(possible_k, function(k) {
  predicted_class <- as.numeric(mse_test.ml > k)
  sum(predicted_class == 1 & y_test.ml == 1)/sum(y_test.ml)
})

qplot(possible_k, recall, geom = "line") +
  labs(x = "Threshold", y = "Recall")
```

```{r}
# fit model

history <- model_ap %>% fit(
  x = x_train, 
  y = x_train, 
  epochs = 100,
  verbose = 0,
  shuffle=TRUE,
  validation_data= list(x_test, x_test)
)


plot(history)


# evaluate the performance of the model
mse.ae.ap <- evaluate(model_ap, x_test, x_test)
mse.ae.ap

hidden_layer_model_ap <- keras_model(inputs = model_ap$input, outputs = get_layer(model_ap, "bottleneck")$output)

hidden_output_ap <- predict(hidden_layer_model_ap, xtrain)

hidden_output_ap_dt <- as.data.frame(hidden_output_ap)


yhat_ml <- predict(model_ap, x_train.ml, verbose=0)

plot(x_train.ml[1,], type = "l", col = "red")
lines(yhat_ml[1,], type = "l", col = "green")


plot(xtrain[1,], type = "l", col = "red", lwd = 3)
lines(yhat_ap[1,], col = "green",type = 'l', lwd = 3)
legend("topleft", legend = c("Original time series","Reconstructed time series"), col=c("red", "green"), pch=1)


par(mfrow=c(2,1))
ggplot(data.frame(PC1 = hidden_output_ap[,1], PC2 = hidden_output_ap[,2]), aes(x = PC1, y = PC2, col = train.ap$status)) + geom_point()+ggtitle("Normal Autoencoder hidden layer reconstruct data plot")


ggplot(data.frame(PC1 = AP_stance_dr[,1], PC2 = AP_stance_dr[,2]), aes(x = PC1, y = PC2,col = train.ap$status)) + geom_point()+ggtitle("FPCA PCs reconstruct data plot")


```


Pretraining LSTM Autoencoders:
data prepare:
```{r}
library(abind)
xtrain_ap <- as.matrix(xtrain)
xtest_ap <- as.matrix(xtest)
x_train_ap_lstm = array_reshape(xtrain_ap, c(dim(xtrain_ap)[1],1, dim(xtrain_ap)[2]))
x_test_ap_lstm = array_reshape(xtest_ap, c(dim(xtest_ap)[1],1, dim(xtest_ap)[2]))

xtrain_ml <- as.matrix(train.ML)
xtest_ml <- as.matrix(test.ML)
x_train_ml_lstm = array_reshape(xtrain_ml, c(dim(xtrain_ml)[1],1, dim(xtrain_ml)[2]))
x_test_ml_lstm = array_reshape(xtest_ml, c(dim(xtest_ml)[1],1, dim(xtest_ml)[2]))

xtrain_v <- as.matrix(train.V)
xtest_v <- as.matrix(test.V)
x_train_v_lstm = array_reshape(xtrain_v, c(dim(xtrain_v)[1],1, dim(xtrain_v)[2]))
x_test_v_lstm = array_reshape(xtest_v, c(dim(xtest_v)[1],1, dim(xtest_v)[2]))

GRFs_train_lstm <- abind(x_train_ap_lstm,x_train_ml_lstm,x_train_v_lstm, along=2)
```



```{r}
set.seed(1)
lstm_model <- keras_model_sequential() 
lstm_model %>% 
  layer_lstm(units = 128, activation = "relu", return_sequences = TRUE, input_shape = c(1,100))%>%
  layer_lstm(units = 32,activation = "relu", return_sequences = FALSE)%>%
  layer_repeat_vector(1)%>%
  layer_lstm(units = 32, activation = "relu",return_sequences = TRUE)%>%
  layer_lstm(units = 128,activation = "relu", return_sequences = TRUE)%>%
  time_distributed(layer_dense(units = 100))

summary(lstm_model)

lstm_model %>% compile(
  loss = "mean_squared_error", 
  optimizer = "adam",
  metrics = c('accuracy')
)

history1 <- lstm_model %>% fit(
  x_train_ap_lstm, x_train_ap_lstm, 
  epochs = 100,
  verbose = 1
)


lstm_model %>% evaluate(x_test_lstm, x_test_lstm)


a <- as.matrix(expand_dims(x_train_lstm[1,,],  axis = 0))

hidden_lstm_model <- keras_model(inputs = lstm_model$input, outputs = get_layer(lstm_model, "lstm_21")$output)

yhat_lstm <- predict(hidden_lstm_model, x_train_ap_lstm, verbose=0)


recons_model <- keras_model(inputs = lstm_model$input, outputs = lstm_model$output)

yhat_lstm_recon <- predict(recons_model, x_train_ap_lstm, verbose=0)

plot(x_train_ap_lstm[1,,], type = "l", col = "red", lwd = 3)
lines(yhat_lstm_recon[1,,], type = "l", col = "green", lwd = 3)
legend("topleft", legend = c("Original time series","Reconstructed time series"), col=c("red", "green"), pch=1)


yhat_lstm <- array_reshape(yhat_lstm, c(10987, 32))

ggplot(data.frame(PC1 = yhat_lstm[,1], PC7 = yhat_lstm[,7]), aes(x = PC1, y = PC7,col = train.ap$status )) + geom_point()+ggtitle("LSTM Autoencoder hidden layer reconstructed data point")

```
