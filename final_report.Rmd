---
title: "SAR PT Project Final Report"
subtitle: "Osteoarthritis Research Analysis"
author: "Boyu Chen, Chen Xu"
date: "May 6, 2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = F,message = F,echo=F,highlight=F)
knitr::opts_chunk$set(fig.width = 12, fig.height = 4,fig.align = "center")
pacman::p_load(
  "fda.usc",
  "fdaoutlier",
  "tidyverse",
  "fda",
  "dplyr",
  "fdapace",
  "plotly",
  "readxl",
  "purrr",
  "keras",
  "caret"
)
```


```{r}
#load data:
ID_info <- read_excel("/Users/xuchen/Desktop/MA679/PTProject_Data/IDinfo.xls")
# Time-normalized data
AP_GRF_stance_N <- read.csv("/Users/xuchen/Desktop/MA679/PTProject_Data/AP_GRF_stance_N.csv", header = FALSE)
ML_GRF_stance_N <- read.csv("/Users/xuchen/Desktop/MA679/PTProject_Data/ML_GRF_stance_N.csv", header = FALSE)
V_GRF_stance_N <- read.csv("/Users/xuchen/Desktop/MA679/PTProject_Data/V_GRF_stance_N.csv", header = FALSE)
```

##Abstract:

    Our client is studying osteoarthritis of the knees by imaging subjects’ walking motion over a series of pressure plates. We reconstructed the force trend with the time series data collected from the pressure plates to have a general insight about the experiment. We then tried to find new important identifiers using different unsupervised dimension reduction models: Functional PCA and traditional Autoencoder. In order to make the model we built more practical, we trained our Autoencoder model to detect outliers and used Precision-Recall curve to determine the threshold. This report consists of four main sections: Introduction, Data and Methods, Model comparison, and Discussion. 

##Introduction:

    Mechanical loading has been implicated in knee osteoarthritis pathogenesis, suggesting that interventions aimed at changing joint loading may be key to reducing the burden of knee osteoarthritis. Our client from the movement & applied imaging lab of Boston University conducted an experiment to study osteoarthritis of the knees by imaging participants’ walking motion over three pieces of pressure plates. During the motion, the pressure plates measure three forces and three moment components. Our clients manually extracted some potential features. What we need to do is try different machine learning methods to reconstruct the motion and see if we can use some new features to describe the motions and study osteoarthritis from a new perspective.
    

##Exploratory data analysis:

###The first step: datasets select

    We select three feature to do analysis: AP_GRFs, ML_GRFs, V_GRFs.AP_GRFs is the anterior-posterior ground reaction force, ML_GRFs is the medial-lateral ground reaction force and V_GRFs is vertical ground reaction force. The reason we select these tree feature is that we want to focus on the GRF in order to have better understanding of experiment and the motion of foot. 

```{r}
mat_ap <- as.matrix(AP_GRF_stance_N)
mat_ml <- as.matrix(ML_GRF_stance_N)
mat_v <- as.matrix(V_GRF_stance_N)

color = rgb(runif(15696,0,1),runif(15696,0,1), runif(15696,0,1))

ml_ts <- function(df){
  plot(df[1,], col = color[1], type = 'l', lwd = 1, ylim = c(-150,200), xlab = "Time point", ylab = "ML_GRF_stance_N")
  for (i in 2:nrow(df)) {
    lines(df[i,], col = color[i], type = 'l', lwd = 1)
  }
}

ap_ts <- function(df){
  plot(df[1,], col = color[1], type = 'l', lwd = 1, ylim = c(-400,400), xlab = "Time point", ylab = "AP_GRF_stance_N")
  for (i in 2:nrow(df)) {
    lines(df[i,], col = color[i], type = 'l', lwd = 1)
  }
}

v_ts <- function(df){
  plot(df[1,], col = color[1], type = 'l', lwd = 1, ylim = c(0,2000), xlab = "Time point", ylab = "V_GRF_stance_N")
  for (i in 2:nrow(df)) {
    lines(df[i,], col = color[i], type = 'l', lwd = 1)
  }
}

```


###Reconstruct the time normalized force plot:

    We first we took look at the trial length distribution for different knees to see if there exists some significant differences between each knee. The two vertical line are median trial length for different knee. As we can see, among the whole participant, the trial length of right foot is a little bit longer than left foot, but there is no significant difference between these two knee. Then we built the time series plot for the ground reaction force of three dimensions to see the trends of these whethere there is some obvious identifiers. 

```{r}
median_trlength <- ID_info %>% group_by(KNEE) %>% summarise(median = median(tr_length))
mean_trlength <- ID_info %>% group_by(KNEE) %>% summarise(mean = mean(tr_length))

# First, we took look at the trial length distribution for different knees 
# to see if there exits some significant difference between each knee.
# The two vertical lines are median trial length for different knee:

trial_length <- ggplot(ID_info, aes(x = tr_length, fill = KNEE))+
  geom_density(alpha = 0.5,adjust = 0.3)+
  geom_vline(data = median_trlength, aes(xintercept = median, color = KNEE), size=1)+
  labs(x= "Trial length",
       subtitle="Trial length distribution for different knee")+
  theme(legend.position="bottom")

trial_length

```

    After we rebuilt the anterior-posterior ground reaction force data, we can find there is an anterior force peak and posterior force peak. After we rebuilt the medial-lateral ground reaction force data, we can find there are three force peaks.After we rebuilt the vertical ground reaction force data, we can find there are two force peaks

```{r}
ap_plot <- ap_ts(mat_ap)
```


```{r}
ml_plot <- ml_ts(mat_ml)
```

```{r}
v_plot <- v_ts(mat_v)
```

    We detect outliers for all 3 time normalized GRFs and label them 1 as outliers and 0 as normal.

| GRF name                                                 | Number of Normal Curves | Number of abnormal Curves(Outliers) |Abnormal Percentage| 
| :--:                                                     |:---:                    |:--:                                 |:--:               |
| Time Normalized Anterior-posterior Ground Reaction Force | 13579                   | 2117                                | 13.5$%$           |
| Time Normalized Medial-lateral Ground Reaction Force     | 13545                   | 2151                                | 13.7$%$           |   
| Time Normalized Vertical Ground Reaction Force           | 13981                   | 1715                                | 10.9$%$           |

    After we use Magnitude-Shape plot based on the directional outlyingness for functional data. We add the labels to three different grand reaction forces and let normal curves equal to 0 and outliers equal to 1.
```{r}

ap_ms <- msplot(dts = AP_GRF_stance_N, plot = T)

ap_outlier_id <- ap_ms$outliers

ap_grf <- AP_GRF_stance_N %>%
  mutate(index = row_number())

# label the outliers

for (i in 1:nrow(ap_grf)) {
  if (ap_grf$index[i] %in% ap_outlier_id == TRUE) {
    ap_grf$status[i] = 1
  }
  if (ap_grf$index[i] %in% ap_outlier_id == FALSE) {
    ap_grf$status[i] = 0
  }
}

ap_grf <- ap_grf[c(102, 1:100)]
```



```{r}
ml_ms <- msplot(dts = ML_GRF_stance_N, plot = T)

ml_outlier_id <- ml_ms$outliers

ml_grf <- ML_GRF_stance_N %>%
  mutate(index = row_number())
```


```{r}
# label the outliers

for (i in 1:nrow(ml_grf)) {
  if (ml_grf$index[i] %in% ml_outlier_id == TRUE) {
    ml_grf$status[i] = 1
  }
  if (ml_grf$index[i] %in% ml_outlier_id == FALSE) {
    ml_grf$status[i] = 0
  }
}

ml_grf <- ml_grf[c(102, 1:100)]



v_ms <- msplot(dts = V_GRF_stance_N, plot = T)

v_outlier_id <- v_ms$outliers


v_grf <- V_GRF_stance_N %>%
  mutate(index = row_number())

# label the outliers

for (i in 1:nrow(v_grf)) {
  if (v_grf$index[i] %in% v_outlier_id == TRUE) {
    v_grf$status[i] = 1
  }
  if (v_grf$index[i] %in% v_outlier_id == FALSE) {
    v_grf$status[i] = 0
  }
}

v_grf <- v_grf[c(102, 1:100)]
```

##Baseline model: Functional PCA
###Introduction to functional PCA:

    Functional principal component analysis (FPCA) is a statistical method for investigating the dominant modes of variation of functional data. Using this method, a random function is represented in the eigenbasis, which is an orthonormal basis of the Hilbert space $L^2$ that consists of the eigenfunctions of the autocovariance operator. FPCA represents functional data in the most parsimonious way, in the sense that when using a fixed number of basis functions, the eigenfunction basis explains more variation than any other basis expansion. FPCA can be applied for representing random functions, or in functional regression and classification. FPCA finds the set of orthogonal principal component functions that maximize the variance along each component.


How to use functional PCA:
1. Calculate the smoothed mean $\hat{\mu}$ (using local linear smoothing) aggregating all the available readings together.
2. Calculate for each curve separately its own raw covariance and then aggregate all these raw covariances to generate the sample raw covariance.
3. Use the off-diagonal elements of the sample raw covariance to estimate the smooth covariance.
4. Perform eigenanalysis on the smoothed covariance to obtain the estimated eigenfunctions $\hat{\phi}$ and eigenvalues $\hat{\lambda}$, then project that smoothed covariance on a positive semi-definite surface (Hall, Müller, and Yao 2008).
5. Use Conditional Expectation (PACE step) to estimate the corresponding scores $\hat{\xi}$. ie.

$$
\hat{\xi}_{i k}=\hat{E}\left[\hat{\xi}_{i k} \mid Y_{i}\right]=\hat{\lambda}_{k} \hat{\phi}_{i k}^{T} \Sigma_{Y_{i}}^{-1}\left(Y_{i}-\hat{\mu}_{i}\right)
$$

    From FPCA, we can make the dimension reduction to our data, from 100 data points to 12 functional principal components for anterior-posterior ground reaction forces.Based on the scree-plot we see that the first three components appear to encapsulate most of the relevant variation. The number of eigen components to reach a 99% fraction of variance explained is 12 but just 7 principal components are enough to reach a 95.0%.
  

```{r}
L_AP <- MakeFPCAInputs(ap_fpca$new, ap_fpca$t, ap_fpca$force)
FPCAdense_AP <- FPCA(L_AP$Ly, L_AP$Lt)

# Plot the FPCA object
plot(FPCAdense_AP)

par(mfrow=c(1,1))
CreateOutliersPlot(FPCAdense_AP, optns = list(K = 12, variant = 'KDE'))
```

    Similar to anterior-posterior ground reaction forces,  we can make the dimension reduction to our data, from 100 data points to 15 functional principal components for medial-lateral ground reaction force .Based on the scree-plot we see that the first three components appear to encapsulate most of the relevant variation. The number of principal components to reach a 99% fraction of variance explained is 15 but just 7 principal components are enough to reach a 95.0%.

```{r}
L_ML <- MakeFPCAInputs(ml_fpca$new, ml_fpca$t, ml_fpca$force)
FPCAdense_ML <- FPCA(L_ML$Ly, L_ML$Lt)

# Plot the FPCA object
plot(FPCAdense_ML)

par(mfrow=c(1,1))
  CreateOutliersPlot(FPCAdense_ML, optns = list(K = 15, variant = 'KDE'))
```

    Similar to medial-lateral ground reaction forces,  we can make the dimension reduction to our data, from 100 data points to 8 functional principal components for vertical ground reaction force .Based on the scree-plot we see that the first three components appear to encapsulate most of the relevant variation. The number of principal components to reach a 99% fraction of variance explained is 8 but just 4 principal components are enough to reach a 95.0%.
    
```{r}
L_V <- MakeFPCAInputs(v_fpca$new, v_fpca$t, v_fpca$force)
FPCAdense_V <- FPCA(L_V$Ly, L_V$Lt)

# Plot the FPCA object
plot(FPCAdense_V)

par(mfrow=c(1,1))
  CreateOutliersPlot(FPCAdense_V, optns = list(K = 8, variant = 'KDE'))
```


## Reconstruction with functional PCA principle components. 
```{r}

```

##Tradictional auto-encoder


    We created 2 functions to help us. The first one gets descriptive statistics about the dataset that are used for scaling. Then we have a function to perform the min-max scaling.

```{r}

#' Gets descriptive statistics for every variable in the dataset.
get_desc <- function(x) {
  map(x, ~list(
    min = min(.x),
    max = max(.x),
    mean = mean(.x),
    sd = sd(.x)
  ))
} 

#' Given a dataset and normalization constants it will create a min-max normalized
#' version of the dataset.
normalization_minmax <- function(x, desc) {
  map2_dfc(x, desc, ~(.x - .y$min)/(.y$max - .y$min))
}
```


Create traditional Autoencoder model for outlier detection

Select ML_GRF_stance_N as example

```{r}

K <- keras::backend()

dt.ml <-  sort(sample(nrow(ml_grf), nrow(ml_grf)*.7))
df_train.ml<-ml_grf[dt.ap,]
df_test.ml<-ml_grf[-dt.ap,]


desc <- df_train.ml %>% 
  select(-status) %>% 
  get_desc()

x_train.ml <- df_train.ml %>%
  select(-status) %>%
  normalization_minmax(desc) %>%
  as.matrix()

x_test.ml <- df_test.ml %>%
  select(-status) %>%
  normalization_minmax(desc) %>%
  as.matrix()

y_train.ml <- df_train.ml$status
y_test.ml <- df_test.ml$status

model_ml <- keras_model_sequential()
model_ml %>%
  layer_dense(units = 50, activation = "relu", input_shape = ncol(x_train.ml)) %>%
  layer_dense(units = 30, activation = "relu", name = "bottleneck") %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dense(units = ncol(x_train.ml))



# summar of model
summary(model_ml) 



# compile model
model_ml %>% compile(
  loss = "mean_squared_error", 
  optimizer = "adam"
)


checkpoint <- callback_model_checkpoint(
  filepath = "model.hdf5", 
  save_best_only = TRUE, 
  period = 1,
  verbose = 1
)

early_stopping <- callback_early_stopping(patience = 5)

model_ml %>% fit(
  x = x_train.ml[y_train.ml == 0,], 
  y = x_train.ml[y_train.ml == 0,], 
  epochs = 100, 
  batch_size = 32,
  validation_data = list(x_test.ml[y_test.ml == 0,], x_test.ml[y_test.ml == 0,]), 
  callbacks = list(checkpoint, early_stopping)
)

loss <- evaluate(model_ml, x = x_test.ml[y_test.ml == 0,], y = x_test.ml[y_test.ml == 0,])
loss


pred_train.ml <- predict(model_ml, x_train.ml)
mse_train.ml <- apply((x_train.ml - pred_train.ml)^2, 1, sum)

pred_test.ml <- predict(model_ml, x_test.ml)
mse_test.ml <- apply((x_test.ml - pred_test.ml)^2, 1, sum)

possible_k <- seq(0, 0.5, length.out = 100)
precision <- sapply(possible_k, function(k) {
  predicted_class <- as.numeric(mse_test.ml > k)
  sum(predicted_class == 1 & y_test.ml == 1)/sum(predicted_class)
})

qplot(possible_k, precision, geom = "line") +
  labs(x = "Threshold", y = "Precision")


recall <- sapply(possible_k, function(k) {
  predicted_class <- as.numeric(mse_test.ml > k)
  sum(predicted_class == 1 & y_test.ml == 1)/sum(y_test.ml)
})

qplot(possible_k, recall, geom = "line") +
  labs(x = "Threshold", y = "Recall")

qplot(recall, precision, geom = "line")+
  labs(x = "Recall", y = "Precision")

fmeasure <- (2 * precision * recall) / (precision + recall)

which.max(fmeasure)

possible_k[which.max(fmeasure)]
```


Model Reconstruction check
```{r}
plot(x_train.ml[1,], type = "l", col = "red")
lines(pred_train.ml[1,], type = "l", col = "green")
```


Results:




Discussion



Citataion
$1.$ Wikimedia Foundation. (2021, October 27). Functional principal component analysis. Wikipedia. Retrieved May 4, 2022, from             https://en.wikipedia.org/wiki/Functional_principal_component_analysis 
$2.$ Shang, H. L. (2013). A survey of functional principal component analysis. AStA Advances in Statistical Analysis, 98(2), 121–142. https://doi.org/10.1007/s10182-013-0213-1 
$3.$Functional PCA in R. (2021, October 1). Retrieved May 4, 2022, from https://cran.r-project.org/web/packages/fdapace/vignettes/fdapaceVig.html 